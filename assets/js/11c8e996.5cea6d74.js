"use strict";(self.webpackChunkdocs=self.webpackChunkdocs||[]).push([[8818],{18706:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var s=i(85893),t=i(11151);const o={title:"TensorRT-LLM",slug:"/guides/providers/tensorrt-llm"},r=void 0,l={id:"guides/providers/tensorrt-llm",title:"TensorRT-LLM",description:"Users with Nvidia GPUs can get 20-40% faster\\* token speeds on their laptop or desktops by using TensorRT-LLM. The greater implication is that you are running FP16, which is also more accurate than quantized models.",source:"@site/docs/guides/providers/tensorrt-llm.md",sourceDirName:"guides/providers",slug:"/guides/providers/tensorrt-llm",permalink:"/guides/providers/tensorrt-llm",draft:!1,unlisted:!1,editUrl:"https://github.com/janhq/jan/tree/dev/docs/docs/guides/providers/tensorrt-llm.md",tags:[],version:"current",lastUpdatedBy:"Henry",lastUpdatedAt:1710497836,formattedLastUpdatedAt:"Mar 15, 2024",frontMatter:{title:"TensorRT-LLM",slug:"/guides/providers/tensorrt-llm"},sidebar:"guidesSidebar",previous:{title:"llama.cpp",permalink:"/guides/providers/llama-cpp"},next:{title:"Extensions",permalink:"/guides/extensions/"}},a={},d=[{value:"Requirements",id:"requirements",level:2},{value:"Install TensorRT-Extension",id:"install-tensorrt-extension",level:2},{value:"Download a Compatible Model",id:"download-a-compatible-model",level:2},{value:"Configure Settings",id:"configure-settings",level:2},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Incompatible Extension vs Engine versions",id:"incompatible-extension-vs-engine-versions",level:3},{value:"Uninstall Extension",id:"uninstall-extension",level:3},{value:"Install Nitro-TensorRT-LLM manually",id:"install-nitro-tensorrt-llm-manually",level:3},{value:"Build your own TensorRT models",id:"build-your-own-tensorrt-models",level:3}];function h(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:["Users with Nvidia GPUs can get ",(0,s.jsx)(n.strong,{children:"20-40% faster* token speeds"})," on their laptop or desktops by using ",(0,s.jsx)(n.a,{href:"https://github.com/NVIDIA/TensorRT-LLM",children:"TensorRT-LLM"}),". The greater implication is that you are running FP16, which is also more accurate than quantized models."]}),"\n",(0,s.jsxs)(n.p,{children:["This guide walks you through how to install Jan's official ",(0,s.jsx)(n.a,{href:"https://github.com/janhq/nitro-tensorrt-llm",children:"TensorRT-LLM Extension"}),". This extension uses ",(0,s.jsx)(n.a,{href:"https://github.com/janhq/nitro-tensorrt-llm",children:"Nitro-TensorRT-LLM"})," as the AI engine, instead of the default ",(0,s.jsx)(n.a,{href:"https://github.com/janhq/nitro",children:"Nitro-Llama-CPP"}),". It includes an efficient C++ server to natively execute the ",(0,s.jsx)(n.a,{href:"https://nvidia.github.io/TensorRT-LLM/gpt_runtime.html",children:"TRT-LLM C++ runtime"}),". It also comes with additional feature and performance improvements like OpenAI compatibility, tokenizer improvements, and queues."]}),"\n",(0,s.jsx)(n.p,{children:"*Compared to using LlamaCPP engine."}),"\n",(0,s.jsxs)(n.admonition,{type:"warning",children:[(0,s.jsx)(n.p,{children:"This feature is only available for Windows users. Linux is coming soon."}),(0,s.jsxs)(n.p,{children:["Additionally, we only prebuilt a few demo models. You can always build your desired models directly on your machine. ",(0,s.jsx)(n.a,{href:"#build-your-own-tensorrt-models",children:"Read here"}),"."]})]}),"\n",(0,s.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"A Windows PC"}),"\n",(0,s.jsx)(n.li,{children:"Nvidia GPU(s): Ada or Ampere series (i.e. RTX 4000s & 3000s). More will be supported soon."}),"\n",(0,s.jsx)(n.li,{children:"3GB+ of disk space to download TRT-LLM artifacts and a Nitro binary"}),"\n",(0,s.jsx)(n.li,{children:"Jan v0.4.9+ or Jan v0.4.8-321+ (nightly)"}),"\n",(0,s.jsxs)(n.li,{children:["Nvidia Driver v535+ (",(0,s.jsx)(n.a,{href:"https://jan.ai/guides/common-error/not-using-gpu/#1-ensure-gpu-mode-requirements",children:"installation guide"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:["CUDA Toolkit v12.2+ (",(0,s.jsx)(n.a,{href:"https://jan.ai/guides/common-error/not-using-gpu/#1-ensure-gpu-mode-requirements",children:"installation guide"}),")"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"install-tensorrt-extension",children:"Install TensorRT-Extension"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Go to Settings > Extensions"}),"\n",(0,s.jsx)(n.li,{children:"Click install next to the TensorRT-LLM Extension"}),"\n",(0,s.jsx)(n.li,{children:"Check that files are correctly downloaded"}),"\n"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-sh",children:"ls ~\\jan\\extensions\\@janhq\\tensorrt-llm-extension\\dist\\bin\r\n# Your Extension Folder should now include `nitro.exe`, among other artifacts needed to run TRT-LLM\n"})}),"\n",(0,s.jsx)(n.h2,{id:"download-a-compatible-model",children:"Download a Compatible Model"}),"\n",(0,s.jsxs)(n.p,{children:["TensorRT-LLM can only run models in ",(0,s.jsx)(n.code,{children:"TensorRT"}),' format. These models, aka "TensorRT Engines", are prebuilt specifically for each target OS+GPU architecture.']}),"\n",(0,s.jsx)(n.p,{children:"We offer a handful of precompiled models for Ampere and Ada cards that you can immediately download and play with:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Restart the application and go to the Hub"}),"\n",(0,s.jsxs)(n.li,{children:["Look for models with the ",(0,s.jsx)(n.code,{children:"TensorRT-LLM"})," label in the recommended models list. Click download. This step might take some time. \ud83d\ude4f"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:"https://hackmd.io/_uploads/rJewrEgRp.png",alt:"image"})}),"\n",(0,s.jsxs)(n.ol,{start:"3",children:["\n",(0,s.jsx)(n.li,{children:"Click use and start chatting!"}),"\n",(0,s.jsx)(n.li,{children:"You may need to allow Nitro in your network"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"alt text",src:i(52803).Z+"",width:"679",height:"428"})}),"\n",(0,s.jsx)(n.admonition,{type:"warning",children:(0,s.jsx)(n.p,{children:"If you are our nightly builds, you may have to reinstall the TensorRT-LLM extension each time you update the app. We're working on better extension lifecyles - stay tuned."})}),"\n",(0,s.jsx)(n.h2,{id:"configure-settings",children:"Configure Settings"}),"\n",(0,s.jsx)(n.p,{children:"You can customize the default parameters for how Jan runs TensorRT-LLM."}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"coming soon"})}),"\n",(0,s.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,s.jsx)(n.h3,{id:"incompatible-extension-vs-engine-versions",children:"Incompatible Extension vs Engine versions"}),"\n",(0,s.jsx)(n.p,{children:"For now, the model versions are pinned to the extension versions."}),"\n",(0,s.jsx)(n.h3,{id:"uninstall-extension",children:"Uninstall Extension"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Quit the app"}),"\n",(0,s.jsx)(n.li,{children:"Go to Settings > Extensions"}),"\n",(0,s.jsx)(n.li,{children:"Delete the entire Extensions folder."}),"\n",(0,s.jsx)(n.li,{children:"Reopen the app, only the default extensions should be restored."}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"install-nitro-tensorrt-llm-manually",children:"Install Nitro-TensorRT-LLM manually"}),"\n",(0,s.jsxs)(n.p,{children:["To manually build the artifacts needed to run the server and TensorRT-LLM, you can reference the source code. ",(0,s.jsx)(n.a,{href:"https://github.com/janhq/nitro-tensorrt-llm?tab=readme-ov-file#quickstart",children:"Read here"}),"."]}),"\n",(0,s.jsx)(n.h3,{id:"build-your-own-tensorrt-models",children:"Build your own TensorRT models"}),"\n",(0,s.jsx)(n.admonition,{type:"info",children:(0,s.jsx)(n.p,{children:"coming soon"})})]})}function c(e={}){const{wrapper:n}={...(0,t.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}},52803:(e,n,i)=>{i.d(n,{Z:()=>s});const s=i.p+"assets/images/image-43682306e0e1318012556ba5111c1b38.png"},11151:(e,n,i)=>{i.d(n,{Z:()=>l,a:()=>r});var s=i(67294);const t={},o=s.createContext(t);function r(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);